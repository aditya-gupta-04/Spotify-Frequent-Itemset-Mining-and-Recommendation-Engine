{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityagupta/pytorch-test/env/lib/python3.8/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import spotipy\n",
    "import spotipy.oauth2 as oauth2\n",
    "from spotipy.oauth2 import SpotifyOAuth,SpotifyClientCredentials\n",
    "\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 22:48:41 WARN Utils: Your hostname, Adityas-MacBook-Air-5.local resolves to a loopback address: 127.0.0.1; using 192.168.100.11 instead (on interface en0)\n",
      "23/12/02 22:48:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/02 22:48:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"Apriori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_to_txt(slice_path, txt_file_path):\n",
    "    data_dict = {}\n",
    "\n",
    "    with open(slice_path) as json_file:\n",
    "    \tdata = json.load(json_file)\n",
    "\n",
    "    txt_file = open(txt_file_path,\"a\")\n",
    "\n",
    "    for playlist in data['playlists']:\n",
    "        line = \"\"\n",
    "        for tracks in playlist['tracks']:\n",
    "            line += tracks['track_uri'].split(':')[-1]\n",
    "            line += \" \"\n",
    "\n",
    "        txt_file.write(line[:-1]+'\\n')\n",
    "\n",
    "    txt_file.close()\n",
    "\n",
    "def create_txt_slices(DATA_FOLDER, TXT_SLICES_FOLDER):\n",
    "\n",
    "    os.mkdir(TXT_SLICES_FOLDER)\n",
    "\n",
    "    for file in tqdm(os.listdir(DATA_FOLDER)):\n",
    "        if file.split('.')[-1] == 'json':\n",
    "            slice_to_txt(os.path.join(DATA_FOLDER, file), os.path.join(TXT_SLICES_FOLDER, f\"slice{file.split('.')[-2]}.txt\"))\n",
    "\n",
    "# create_txt_slices('data100', 'data_txt_slices_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_k_tuples(candidate_k_tuples, min_support, shared_itemset):\n",
    "\n",
    "    def calc_support(x):\n",
    "        x_support = len([1 for t in shared_itemset.value if x.issubset(t)])\n",
    "        return (x, x_support)\n",
    "\n",
    "    frequent_k_tuples = sc.parallelize(candidate_k_tuples, 32).map(calc_support).filter(lambda x: x[1] > min_support).collect()\n",
    "    return frequent_k_tuples\n",
    "\n",
    "def construct_candidate_k_tuples(frequent_k_tuples): \n",
    "    n = len(frequent_k_tuples)\n",
    "    k = len(frequent_k_tuples[0][0])\n",
    "\n",
    "    new_candidate_tuples = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            itemset1 = list(frequent_k_tuples[i][0])\n",
    "            itemset2 = list(frequent_k_tuples[j][0])\n",
    "\n",
    "            if itemset1[:-1] == itemset2[:-1]:\n",
    "                new_candidate_tuples.append(frequent_k_tuples[i][0] | {itemset2[-1]})\n",
    "\n",
    "    return new_candidate_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_dataload(input_dir):\n",
    "    \n",
    "    data = sc.textFile(input_dir)\n",
    "    itemset = data.map(lambda line: sorted(line.strip().split(' ')))\n",
    "\n",
    "    return itemset\n",
    "\n",
    "def apriori_support_estimates(itemset):\n",
    "\n",
    "    shared_itemset = sc.broadcast(itemset.map(lambda x: set(x)).collect())\n",
    "    candidate_k_tuples = itemset.flatMap(lambda x: set(x)).distinct().collect()\n",
    "    candidate_k_tuples = [{x} for x in candidate_k_tuples]\n",
    "\n",
    "    for sup_ratio in [0.005, 0.01, 0.015, 0.02, 0.05, 0.10]:\n",
    "        support = sup_ratio * itemset.count()\n",
    "        frequent_k_tuples = get_frequent_k_tuples(candidate_k_tuples, support, shared_itemset)\n",
    "        \n",
    "        print(f\"Support - {sup_ratio*100}%  {support} | # of top tracks considered - {len(frequent_k_tuples)}\")\n",
    "\n",
    "def apriori(itemset, support_threshold_ratio):\n",
    "\n",
    "    shared_itemset = sc.broadcast(itemset.map(lambda x: set(x)).collect())\n",
    "    support_threshold = itemset.count()*support_threshold_ratio\n",
    "    start = time.time()\n",
    "\n",
    "    k = 0\n",
    "    candidate_k_tuples = itemset.flatMap(lambda x: set(x)).distinct().collect()\n",
    "    candidate_k_tuples = [{x} for x in candidate_k_tuples]\n",
    "\n",
    "    while len(candidate_k_tuples) > 0:\n",
    "\n",
    "        frequent_k_tuples = get_frequent_k_tuples(candidate_k_tuples, support_threshold, shared_itemset)\n",
    "        k += 1\n",
    "        print(f\"k = {k} | freq_itemsets = {len(frequent_k_tuples)}\")\n",
    "        print(f\"Time elapsed = {time.time() - start}\")\n",
    "\n",
    "        if len(frequent_k_tuples) == 0:\n",
    "            break\n",
    "\n",
    "        candidate_k_tuples = construct_candidate_k_tuples(frequent_k_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=11922Kb max_used=11935Kb free=119149Kb\n",
      " bounds [0x0000000104424000, 0x0000000104fe4000, 0x000000010c424000]\n",
      " total_blobs=4720 nmethods=3912 adapters=723\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 | freq_itemsets = 662\n",
      "Time elapsed = 12.636770009994507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2 | freq_itemsets = 112\n",
      "Time elapsed = 30.464622020721436\n",
      "k = 3 | freq_itemsets = 8\n",
      "Time elapsed = 30.649697065353394\n",
      "k = 4 | freq_itemsets = 0\n",
      "Time elapsed = 30.785190105438232\n"
     ]
    }
   ],
   "source": [
    "itemsets = apriori_dataload('data_txt_slices')\n",
    "# # apriori_support_estimates(itemsets)\n",
    "apriori(itemsets, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP-Growth Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"AprioriDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemsets Created\n",
      "Removed duplicates\n",
      "PySpark DF created\n",
      "+--------------------+\n",
      "|           playlists|\n",
      "+--------------------+\n",
      "|[20aGiCMoN89NFAEu...|\n",
      "|[2faSzprTWJ7L1EkZ...|\n",
      "|[6ITDAE1VFqNtNBJ5...|\n",
      "|[17txou7v6Jxrwm4S...|\n",
      "|[6hqt1z34Oz0OZtSf...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemset = apriori_dataload('data_txt_slices')\n",
    "print(\"Itemsets Created\")\n",
    "itemset = itemset.map(lambda x : set(x)).map(lambda x : list(x))\n",
    "print(\"Removed duplicates\")\n",
    "itemset = itemset.map(lambda x: (x,)).toDF().withColumnRenamed('_1', 'playlists')\n",
    "print(\"PySpark DF created\")\n",
    "itemset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|[28cnXtME493VX9NO...|  36|\n",
      "|[45yEy5WJywhJ3sDI...|  55|\n",
      "|[3f7gYMirBEKuc572...|  47|\n",
      "|[7zFXmv6vqI4qOt4y...|  37|\n",
      "|[2b9lp5A6CqSzwOrB...|  37|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"playlists\", minSupport=0.005)\n",
    "model = fpGrowth.fit(itemset)\n",
    "\n",
    "freq_itemsets = model.freqItemsets\n",
    "freq_itemsets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "freq_itemsets = freq_itemsets.withColumn('k', F.size('items'))\n",
    "df = freq_itemsets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
